{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DATA_init/nabirds/images_128/0991\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-989e29ce8856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'images_128/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'images_128/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    config_info = json.load(file)\n",
    "\n",
    "DATA_init = config_info[\"DATA_init\"] \n",
    "#DATA_init = '/DATA_init/' #Location where NABirds dataset is located. For instance, \"DATA_init+'nabirds/'\" should be the path of the NABirds dataset.\n",
    "FOLDER_init = config_info[\"FOLDER_init\"]\n",
    "#FOLDER_init = '/FOLDER_init/' #Location where this repogistory \"ProxyDR\" is located.\n",
    "\n",
    "DATA_PATH = DATA_init+'nabirds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    os.mkdir(DATA_PATH+'images_128') \n",
    "except:\n",
    "    pass\n",
    "\n",
    "cls_nm_list=os.listdir(DATA_PATH+'images')\n",
    "#Generates folders for 128x128 resized images\n",
    "for l in cls_nm_list:\n",
    "    os.mkdir(DATA_PATH+'images_128/'+l) \n",
    "    print(DATA_PATH+'images_128/'+l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_list=[]\n",
    "cls_list=[]\n",
    "data_info=[]\n",
    "cnt=0\n",
    "for root, directories, filenames in os.walk(DATA_PATH): \n",
    "    directories.sort()\n",
    "    chk_root = not (base_PATH+'/_') in root\n",
    "    chk_jpg = [fnm for fnm in filenames if fnm.split('.j')[-1]=='pg'] #Only check '.jpg' extension files. (It ignores '.JPG' extension files using capital letters)\n",
    "    chk_nonjpg = [fnm for fnm in filenames if fnm.split('.j')[-1]!='pg']\n",
    "    #print(root, filenames)\n",
    "    \n",
    "    #The following four lines will replace '.JPG' extension files to '.jpg' extension files. \n",
    "    if (len(chk_nonjpg)>=1):\n",
    "        for i in range(len(chk_nonjpg)):\n",
    "            print(root+'/'+chk_nonjpg[i])\n",
    "            os.rename(root+'/'+chk_nonjpg[i], root+'/'+chk_nonjpg[i].replace('.JPG','.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /FOLDER_init/ProxyDR/nabirds_info.csv does not exist: '/FOLDER_init/ProxyDR/nabirds_info.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-35b900ebf333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdata_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLDER_init\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'ProxyDR/nabirds_info.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdata_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLDER_init\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'ProxyDR/nabirds_cls2.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /FOLDER_init/ProxyDR/nabirds_info.csv does not exist: '/FOLDER_init/ProxyDR/nabirds_info.csv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(0) #You may want to change GPU number\n",
    "\n",
    "dataset='nabirds'\n",
    "\n",
    "gray = False\n",
    "    \n",
    "data_info = pd.read_csv(FOLDER_init+'ProxyDR/nabirds_info.csv')\n",
    "data_cls = pd.read_csv(FOLDER_init+'ProxyDR/nabirds_cls2.csv',delimiter='|')\n",
    "\n",
    "\n",
    "N= len(data_info) #Number of images: 48562\n",
    "\n",
    "\n",
    "N_train = int(0.7*N) #70%\n",
    "N_val = int(0.1*N) #10%\n",
    "N_test = N-N_train-N_val #20%\n",
    "\n",
    "n_classes = len(data_cls) #Number of clasees\n",
    "\n",
    "resize=128 \n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "        \n",
    "        \n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "\n",
    "class NabirdsDataset(Dataset):\n",
    "    def __init__(self, info_path, cls_path, transform=None):\n",
    "        self.data_info = pd.read_csv(info_path)\n",
    "        self.data_cls = pd.read_csv(cls_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.data_info.iloc[index,0]\n",
    "        img_path = img_path.replace('images_128/','images/')\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        cls = self.data_info.iloc[index,1]\n",
    "        \n",
    "        label = torch.tensor(int(self.data_cls.loc[self.data_cls['Class']==cls,'Number']))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)    \n",
    "            \n",
    "        img = TF.resize(img, size=[resize, resize]) #size=[128,128]) \n",
    "        img.save(img_path.replace('images/','images_'+str(resize)+'/')) #Save resized images \n",
    "        return (img, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "nabirds_dataset = NabirdsDataset(info_path=FOLDER_init+'ProxyDR/'+dataset+'_info.csv', \n",
    "                              cls_path=FOLDER_init+'ProxyDR/'+dataset+'_cls.csv',transform=None)\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(nabirds_dataset, lengths=[N_train, N_val, N_test], generator=torch.Generator().manual_seed(1)) \n",
    "\n",
    "\n",
    "class MapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, map_fn):\n",
    "        self.dataset = dataset\n",
    "        self.map = map_fn\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label= self.dataset[index]\n",
    "        img = self.map(img)\n",
    "        return (img, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "AUG=False\n",
    "train_set_tf = MapDataset(train_set, transforms.ToTensor())\n",
    "val_set_tf = MapDataset(val_set,  transforms.ToTensor())\n",
    "test_set_tf = MapDataset(test_set,  transforms.ToTensor())\n",
    "\n",
    "\n",
    "#trainloader = DataLoader(train_set_tf, batch_size=32, shuffle=True)\n",
    "trainloader2 = DataLoader(MapDataset(train_set, transforms.ToTensor()), batch_size=200, shuffle=True)\n",
    "\n",
    "#valloader = DataLoader(val_set_tf, batch_size=32)\n",
    "valloader2 = DataLoader(MapDataset(val_set, transforms.ToTensor()), batch_size=200, shuffle=True) \n",
    "\n",
    "#testloader = DataLoader(test_set_tf, batch_size=32)\n",
    "testloader2 = DataLoader(test_set_tf, batch_size=200, shuffle=True) \n",
    "\n",
    "\n",
    "'''Analyze training set'''\n",
    "use_cuda = torch.cuda.is_available()\n",
    "labels_np=np.ones(0)*np.nan\n",
    "t1=time.time()\n",
    "for i, data in enumerate(trainloader2):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    if i%10==0:\n",
    "        t2=time.time()\n",
    "        print(i, 100*i/len(trainloader2))\n",
    "        print('Spend time:', t2-t1)\n",
    "    inputs, labels = data\n",
    "\n",
    "    if use_cuda:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    labels_np=np.concatenate([labels_np, labels.cpu().numpy()],axis=0)\n",
    "        \n",
    "print('Finished for training data\\n\\n')\n",
    "for i, data in enumerate(valloader2):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    if i%10==0:\n",
    "        t2=time.time()\n",
    "        print(i, 100*i/len(valloader2))\n",
    "        print('Spend time:', t2-t1)\n",
    "    inputs, labels = data\n",
    "    \n",
    "print('Finished for validation data\\n\\n')\n",
    "for i, data in enumerate(testloader2):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    if i%10==0:\n",
    "        t2=time.time()\n",
    "        print(i, 100*i/len(testloader2))\n",
    "        print('Spend time:', t2-t1)\n",
    "    inputs, labels = data\n",
    "        \n",
    "print('Finished for test data\\n\\n')\n",
    "\n",
    "cls_unq, cls_cnt= np.unique(labels_np,return_counts=True)\n",
    "log_cls_prior = np.log(cls_cnt/np.sum(cls_cnt))  #np.log(cls_cnt)-np.mean(np.log(cls_cnt))\n",
    "inputs.shape, cls_unq, cls_cnt, len(cls_unq), log_cls_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
